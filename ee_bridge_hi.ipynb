{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb2b7f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /home/user/anaconda3/envs/rizwan/lib/python3.11/site-packages/jupyter-1.0.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: fuzzywuzzy in /home/user/anaconda3/envs/rizwan/lib/python3.11/site-packages (0.18.0)\n",
      "Requirement already satisfied: python-Levenshtein in /home/user/anaconda3/envs/rizwan/lib/python3.11/site-packages (0.23.0)\n",
      "Requirement already satisfied: deep-translator in /home/user/anaconda3/envs/rizwan/lib/python3.11/site-packages (1.11.4)\n",
      "Requirement already satisfied: Levenshtein==0.23.0 in /home/user/anaconda3/envs/rizwan/lib/python3.11/site-packages (from python-Levenshtein) (0.23.0)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.1.0 in /home/user/anaconda3/envs/rizwan/lib/python3.11/site-packages (from Levenshtein==0.23.0->python-Levenshtein) (3.5.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /home/user/anaconda3/envs/rizwan/lib/python3.11/site-packages (from deep-translator) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /home/user/anaconda3/envs/rizwan/lib/python3.11/site-packages (from deep-translator) (2.31.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/user/anaconda3/envs/rizwan/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/anaconda3/envs/rizwan/lib/python3.11/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/anaconda3/envs/rizwan/lib/python3.11/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/anaconda3/envs/rizwan/lib/python3.11/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/anaconda3/envs/rizwan/lib/python3.11/site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2023.7.22)\n",
      "\u001b[33mWARNING: Skipping /home/user/anaconda3/envs/rizwan/lib/python3.11/site-packages/jupyter-1.0.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install fuzzywuzzy python-Levenshtein deep-translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f39db77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import nltk\n",
    "import difflib\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForTokenClassification,\n",
    "    get_scheduler\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e03162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IHQID_1mg_train_path = \"indic-health-demo/Dataset/IHQID-1mg/train.csv\"\n",
    "# IHQID_1mg_test_path = \"indic-health-demo/Dataset/IHQID-1mg/test.csv\"\n",
    "\n",
    "IHQID_WebMD_train_path = \"indic-health-demo/Dataset/IHQID-WebMD/train.csv\"\n",
    "IHQID_WebMD_test_path = \"indic-health-demo/Dataset/IHQID-WebMD/test.csv\"\n",
    "\n",
    "\n",
    "IHQID_train = pd.read_csv(IHQID_WebMD_train_path)[['question_bengali', 'disease_bengali', 'drug_bengali', 'treatment_bengali']]\n",
    "IHQID_test = pd.read_csv(IHQID_WebMD_test_path)[['question_bengali', 'disease_bengali', 'drug_bengali', 'treatment_bengali']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b6a43d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 720/720 [31:22<00:00,  2.61s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 241/241 [10:23<00:00,  2.59s/it]\n"
     ]
    }
   ],
   "source": [
    "google_translator_bridge_language = GoogleTranslator(source='bn', target='hi')\n",
    "google_translator = GoogleTranslator(source='hi', target='en')\n",
    "\n",
    "for index in tqdm(range(len(IHQID_train['question_bengali']))):\n",
    "    IHQID_train['question_bengali'][index] = google_translator_bridge_language.translate(IHQID_train['question_bengali'][index])\n",
    "    IHQID_train['question_bengali'][index] = google_translator.translate(IHQID_train['question_bengali'][index])\n",
    "    \n",
    "    if type(IHQID_train['disease_bengali'][index]) is not float:\n",
    "        IHQID_train['disease_bengali'][index] = google_translator_bridge_language.translate(IHQID_train['disease_bengali'][index])\n",
    "        IHQID_train['disease_bengali'][index] = google_translator.translate(IHQID_train['disease_bengali'][index])\n",
    "        \n",
    "    if type(IHQID_train['drug_bengali'][index]) is not float:\n",
    "        IHQID_train['drug_bengali'][index] = google_translator_bridge_language.translate(IHQID_train['drug_bengali'][index])\n",
    "        IHQID_train['drug_bengali'][index] = google_translator.translate(IHQID_train['drug_bengali'][index])\n",
    "        \n",
    "    if type(IHQID_train['treatment_bengali'][index]) is not float:\n",
    "        IHQID_train['treatment_bengali'][index] = google_translator_bridge_language.translate(IHQID_train['treatment_bengali'][index])\n",
    "        IHQID_train['treatment_bengali'][index] = google_translator.translate(IHQID_train['treatment_bengali'][index])\n",
    "    \n",
    "for index in tqdm(range(len(IHQID_test['question_bengali']))):\n",
    "    IHQID_test['question_bengali'][index] = google_translator_bridge_language.translate(IHQID_test['question_bengali'][index])\n",
    "    IHQID_test['question_bengali'][index] = google_translator.translate(IHQID_test['question_bengali'][index])\n",
    "    \n",
    "    if type(IHQID_test['disease_bengali'][index]) is not float:\n",
    "        IHQID_test['disease_bengali'][index] = google_translator_bridge_language.translate(IHQID_test['disease_bengali'][index])\n",
    "        IHQID_test['disease_bengali'][index] = google_translator.translate(IHQID_test['disease_bengali'][index])\n",
    "        \n",
    "    if type(IHQID_test['drug_bengali'][index]) is not float:\n",
    "        IHQID_test['drug_bengali'][index] = google_translator_bridge_language.translate(IHQID_test['drug_bengali'][index])\n",
    "        IHQID_test['drug_bengali'][index] = google_translator.translate(IHQID_test['drug_bengali'][index])\n",
    "        \n",
    "    if type(IHQID_test['treatment_bengali'][index]) is not float:\n",
    "        IHQID_test['treatment_bengali'][index] = google_translator_bridge_language.translate(IHQID_test['treatment_bengali'][index])\n",
    "        IHQID_test['treatment_bengali'][index] = google_translator.translate(IHQID_test['treatment_bengali'][index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d61386",
   "metadata": {},
   "source": [
    "# GENERATING TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baa674eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_sequence(tokenized_sentence, tokenized_entity, entity, tag_list):\n",
    "    contiguous_indices = {\n",
    "        \"similarity\": 0.0,\n",
    "        \"index_start\": -1,\n",
    "        \"index_end\": -1\n",
    "    }\n",
    "    target_length = len(tokenized_entity)\n",
    "    sentence_length = len(tokenized_sentence)\n",
    "\n",
    "    for i in range(sentence_length - target_length + 1):\n",
    "        # Target length = target length\n",
    "        fuzz_ratio = fuzz.ratio(tokenized_entity, tokenized_sentence[i:i + target_length])\n",
    "        \n",
    "        if fuzz_ratio >= 80 and contiguous_indices[\"similarity\"] < fuzz_ratio:\n",
    "            contiguous_indices[\"similarity\"] = fuzz_ratio\n",
    "            contiguous_indices[\"index_start\"] = i\n",
    "            contiguous_indices[\"index_end\"] = i + target_length - 1\n",
    "            \n",
    "        # Target length = target length - 1\n",
    "        fuzz_ratio = fuzz.ratio(tokenized_entity, tokenized_sentence[i:i + target_length - 1])\n",
    "        \n",
    "        if fuzz_ratio >= 80 and contiguous_indices[\"similarity\"] < fuzz_ratio:\n",
    "            # To take care of when tokenization increased the number of tokens [Eg - (1000mg/ mg) vs (1000mg/mg)]\n",
    "            contiguous_indices[\"similarity\"] = fuzz_ratio\n",
    "            contiguous_indices[\"index_start\"] = i\n",
    "            contiguous_indices[\"index_end\"] = i + target_length - 2\n",
    "            \n",
    "    tag_list[contiguous_indices[\"index_start\"]] = \"B-\" + entity\n",
    "    for index in range(contiguous_indices[\"index_start\"] + 1, contiguous_indices[\"index_end\"] + 1):\n",
    "        tag_list[index] = \"I-\" + entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "513a34d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IHQID_train['tags_bengali'] = IHQID_train['question_bengali']\n",
    "\n",
    "for i in range(len(IHQID_train)):\n",
    "    \n",
    "    tokenized_sentence = word_tokenize(IHQID_train['question_bengali'][i].lower())\n",
    "    \n",
    "    tag_list = ['O' for token in tokenized_sentence]\n",
    "    \n",
    "    tokenized_diseases = []\n",
    "    tokenized_drugs = []\n",
    "    tokenized_treatments = []\n",
    "    \n",
    "    if type(IHQID_train['disease_bengali'][i]) is not float:\n",
    "        tokenized_diseases = [word_tokenize(entity.lower()) for entity in IHQID_train['disease_bengali'][i].split(',')]\n",
    "        for tokenized_disease in tokenized_diseases:\n",
    "            get_common_sequence(tokenized_sentence, tokenized_disease, \"disease\", tag_list)\n",
    "    else:\n",
    "        assert(math.isnan(IHQID_train['disease_bengali'][i]))\n",
    "        \n",
    "    if type(IHQID_train['drug_bengali'][i]) is not float:\n",
    "        tokenized_drugs = [word_tokenize(entity.lower()) for entity in IHQID_train['drug_bengali'][i].split(',')]\n",
    "        for tokenized_drug in tokenized_drugs:\n",
    "            get_common_sequence(tokenized_sentence, tokenized_drug, \"drug\", tag_list)\n",
    "    else:\n",
    "        assert(math.isnan(IHQID_train['drug_bengali'][i]))\n",
    "    \n",
    "    if type(IHQID_train['treatment_bengali'][i]) is not float:\n",
    "        tokenized_treatments = [word_tokenize(entity.lower()) for entity in IHQID_train['treatment_bengali'][i].split(',')]\n",
    "        for tokenized_treatment in tokenized_treatments:\n",
    "            get_common_sequence(tokenized_sentence, tokenized_treatment, \"treatment\", tag_list)\n",
    "    else:\n",
    "        assert(math.isnan(IHQID_train['treatment_bengali'][i]))\n",
    "\n",
    "    IHQID_train['question_bengali'][i] = tokenized_sentence\n",
    "    IHQID_train['tags_bengali'][i] = tag_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f34f4f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "IHQID_test['tags_bengali'] = IHQID_test['question_bengali']\n",
    "\n",
    "for i in range(len(IHQID_test)):\n",
    "    \n",
    "    tokenized_sentence = word_tokenize(IHQID_test['question_bengali'][i].lower())\n",
    "    \n",
    "    tag_list = ['O' for token in tokenized_sentence]\n",
    "    \n",
    "    tokenized_diseases = []\n",
    "    tokenized_drugs = []\n",
    "    tokenized_treatments = []\n",
    "    \n",
    "    if type(IHQID_test['disease_bengali'][i]) is not float:\n",
    "        tokenized_diseases = [word_tokenize(entity.lower()) for entity in IHQID_test['disease_bengali'][i].split(',')]\n",
    "        for tokenized_disease in tokenized_diseases:\n",
    "            get_common_sequence(tokenized_sentence, tokenized_disease, \"disease\", tag_list)\n",
    "    else:\n",
    "        assert(math.isnan(IHQID_test['disease_bengali'][i]))\n",
    "        \n",
    "    if type(IHQID_test['drug_bengali'][i]) is not float:\n",
    "        tokenized_drugs = [word_tokenize(entity.lower()) for entity in IHQID_test['drug_bengali'][i].split(',')]\n",
    "        for tokenized_drug in tokenized_drugs:\n",
    "            get_common_sequence(tokenized_sentence, tokenized_drug, \"drug\", tag_list)\n",
    "    else:\n",
    "        assert(math.isnan(IHQID_test['drug_bengali'][i]))\n",
    "    \n",
    "    if type(IHQID_test['treatment_bengali'][i]) is not float:\n",
    "        tokenized_treatments = [word_tokenize(entity.lower()) for entity in IHQID_test['treatment_bengali'][i].split(',')]\n",
    "        for tokenized_treatment in tokenized_treatments:\n",
    "            get_common_sequence(tokenized_sentence, tokenized_treatment, \"treatment\", tag_list)\n",
    "    else:\n",
    "        assert(math.isnan(IHQID_test['treatment_bengali'][i]))\n",
    "\n",
    "    IHQID_test['question_bengali'][i] = tokenized_sentence\n",
    "    IHQID_test['tags_bengali'][i] = tag_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2ea4382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_bengali</th>\n",
       "      <th>disease_bengali</th>\n",
       "      <th>drug_bengali</th>\n",
       "      <th>treatment_bengali</th>\n",
       "      <th>tags_bengali</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[what, is, nystatin, prescribed, for, ?]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nystatin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[O, O, B-drug, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[can, showering, after, sex, prevent, me, from...</td>\n",
       "      <td>pregnant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-disease, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[percocet, causes, weight, gain]</td>\n",
       "      <td>weight gain</td>\n",
       "      <td>Percocet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[B-drug, O, B-disease, I-disease]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[can, 2, or, 2, 1/2, glasses, of, wine, a, day...</td>\n",
       "      <td>high blood pressure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-disease, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[can, too, much, buttermilk, cause, thrush, ?]</td>\n",
       "      <td>thrush</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[O, O, O, O, O, B-disease, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    question_bengali      disease_bengali  \\\n",
       "0           [what, is, nystatin, prescribed, for, ?]                  NaN   \n",
       "1  [can, showering, after, sex, prevent, me, from...             pregnant   \n",
       "2                   [percocet, causes, weight, gain]          weight gain   \n",
       "3  [can, 2, or, 2, 1/2, glasses, of, wine, a, day...  high blood pressure   \n",
       "4     [can, too, much, buttermilk, cause, thrush, ?]               thrush   \n",
       "\n",
       "  drug_bengali treatment_bengali  \\\n",
       "0     nystatin               NaN   \n",
       "1          NaN               NaN   \n",
       "2     Percocet               NaN   \n",
       "3          NaN               NaN   \n",
       "4          NaN               NaN   \n",
       "\n",
       "                                        tags_bengali  \n",
       "0                            [O, O, B-drug, O, O, O]  \n",
       "1             [O, O, O, O, O, O, O, O, B-disease, O]  \n",
       "2                  [B-drug, O, B-disease, I-disease]  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, B-disease, I...  \n",
       "4                      [O, O, O, O, O, B-disease, O]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IHQID_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "820677b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_bengali</th>\n",
       "      <th>disease_bengali</th>\n",
       "      <th>drug_bengali</th>\n",
       "      <th>treatment_bengali</th>\n",
       "      <th>tags_bengali</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>[can, an, insurance, company, be, required, to...</td>\n",
       "      <td>pregnancy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-disease, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>[how, can, i, use, duct, tape, to, get, rid, o...</td>\n",
       "      <td>wart</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, B-disease, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>[bell, 's, palsy, what, facial, exercises, can...</td>\n",
       "      <td>Bell's palsy facial paralysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Exercise</td>\n",
       "      <td>[B-disease, I-disease, I-disease, I-disease, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>[is, prenatal, ultrasound, safe, ?]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ultrasound</td>\n",
       "      <td>[O, O, B-treatment, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>[how, can, i, reduce, inguinal, hernia, sympto...</td>\n",
       "      <td>inguinal hernia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Surgery</td>\n",
       "      <td>[O, O, O, O, B-disease, I-disease, O, O, B-tre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      question_bengali  \\\n",
       "715  [can, an, insurance, company, be, required, to...   \n",
       "716  [how, can, i, use, duct, tape, to, get, rid, o...   \n",
       "717  [bell, 's, palsy, what, facial, exercises, can...   \n",
       "718                [is, prenatal, ultrasound, safe, ?]   \n",
       "719  [how, can, i, reduce, inguinal, hernia, sympto...   \n",
       "\n",
       "                   disease_bengali drug_bengali treatment_bengali  \\\n",
       "715                      pregnancy          NaN               NaN   \n",
       "716                           wart          NaN               NaN   \n",
       "717  Bell's palsy facial paralysis          NaN          Exercise   \n",
       "718                            NaN          NaN        ultrasound   \n",
       "719                inguinal hernia          NaN           Surgery   \n",
       "\n",
       "                                          tags_bengali  \n",
       "715  [O, O, O, O, O, O, O, O, O, O, O, B-disease, O...  \n",
       "716       [O, O, O, O, O, O, O, O, O, O, B-disease, O]  \n",
       "717  [B-disease, I-disease, I-disease, I-disease, I...  \n",
       "718                          [O, O, B-treatment, O, O]  \n",
       "719  [O, O, O, O, B-disease, I-disease, O, O, B-tre...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IHQID_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "267a99c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_bengali</th>\n",
       "      <th>disease_bengali</th>\n",
       "      <th>drug_bengali</th>\n",
       "      <th>treatment_bengali</th>\n",
       "      <th>tags_bengali</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[how, common, is, pregnancy, after, hysterecto...</td>\n",
       "      <td>pregnancy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hysterectomy</td>\n",
       "      <td>[O, O, O, B-disease, O, B-treatment, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[how, effective, are, generic, thyroid, medica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>generic thyroid medicine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[O, O, O, B-drug, I-drug, I-drug, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[is, singulair, a, corticosteroid, ?]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Singulair, corticosteroids</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[O, B-drug, O, B-drug, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[can, you, take, advil, with, homeopathic, ear...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Advil, homeopathic ear drops</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[O, O, O, B-drug, O, B-drug, I-drug, I-drug, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[is, bell, 's, palsy, contagious, ?]</td>\n",
       "      <td>Bell's palsy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[O, B-disease, I-disease, I-disease, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    question_bengali disease_bengali  \\\n",
       "0  [how, common, is, pregnancy, after, hysterecto...       pregnancy   \n",
       "1  [how, effective, are, generic, thyroid, medica...             NaN   \n",
       "2              [is, singulair, a, corticosteroid, ?]             NaN   \n",
       "3  [can, you, take, advil, with, homeopathic, ear...             NaN   \n",
       "4               [is, bell, 's, palsy, contagious, ?]    Bell's palsy   \n",
       "\n",
       "                   drug_bengali treatment_bengali  \\\n",
       "0                           NaN      hysterectomy   \n",
       "1      generic thyroid medicine               NaN   \n",
       "2    Singulair, corticosteroids               NaN   \n",
       "3  Advil, homeopathic ear drops               NaN   \n",
       "4                           NaN               NaN   \n",
       "\n",
       "                                      tags_bengali  \n",
       "0          [O, O, O, B-disease, O, B-treatment, O]  \n",
       "1             [O, O, O, B-drug, I-drug, I-drug, O]  \n",
       "2                        [O, B-drug, O, B-drug, O]  \n",
       "3  [O, O, O, B-drug, O, B-drug, I-drug, I-drug, O]  \n",
       "4       [O, B-disease, I-disease, I-disease, O, O]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IHQID_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9db7337e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_bengali</th>\n",
       "      <th>disease_bengali</th>\n",
       "      <th>drug_bengali</th>\n",
       "      <th>treatment_bengali</th>\n",
       "      <th>tags_bengali</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>[how, will, obamacare, affect, medicare, recip...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>[what, is, the, role, of, mptp, in, the, study...</td>\n",
       "      <td>Parkinson's disease</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mptp</td>\n",
       "      <td>[O, O, O, O, O, B-treatment, O, O, O, O, B-dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>[is, it, safe, to, use, botox, for, frown, lin...</td>\n",
       "      <td>frowning, wrinkling</td>\n",
       "      <td>botox</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[O, O, O, O, O, B-drug, O, B-disease, O, O, B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>[when, should, i, have, an, ultrasound, during...</td>\n",
       "      <td>during pregnancy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ultrasound</td>\n",
       "      <td>[O, O, O, O, O, B-treatment, B-disease, I-dise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>[does, nucinta, have, tylenol, ?]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nucinta, Tylenol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[O, B-drug, O, B-drug, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      question_bengali      disease_bengali  \\\n",
       "236  [how, will, obamacare, affect, medicare, recip...                  NaN   \n",
       "237  [what, is, the, role, of, mptp, in, the, study...  Parkinson's disease   \n",
       "238  [is, it, safe, to, use, botox, for, frown, lin...  frowning, wrinkling   \n",
       "239  [when, should, i, have, an, ultrasound, during...     during pregnancy   \n",
       "240                  [does, nucinta, have, tylenol, ?]                  NaN   \n",
       "\n",
       "         drug_bengali treatment_bengali  \\\n",
       "236               NaN               NaN   \n",
       "237               NaN              mptp   \n",
       "238             botox               NaN   \n",
       "239               NaN        ultrasound   \n",
       "240  Nucinta, Tylenol               NaN   \n",
       "\n",
       "                                          tags_bengali  \n",
       "236  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "237  [O, O, O, O, O, B-treatment, O, O, O, O, B-dis...  \n",
       "238  [O, O, O, O, O, B-drug, O, B-disease, O, O, B-...  \n",
       "239  [O, O, O, O, O, B-treatment, B-disease, I-dise...  \n",
       "240                          [O, B-drug, O, B-drug, O]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IHQID_test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934f37aa",
   "metadata": {},
   "source": [
    "# TRAINING AND TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37cd8db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label__ = {\n",
    "    'O': 0,\n",
    "    'B-treatment': 1,\n",
    "    'I-treatment': 2,\n",
    "    'B-disease': 3,\n",
    "    'I-disease': 4,\n",
    "    'B-drug': 5,\n",
    "    'I-drug': 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48aecec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "model_checkpoint = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "\n",
    "hyper_parameters = {\n",
    "    'batch_size': 8,\n",
    "    'lr': 3e-5,\n",
    "    'epochs': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15f1a18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(model_checkpoint, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "091a89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_queries(question, tag): \n",
    "    \n",
    "    tokenized_input = tokenizer(question, max_length=300, padding='max_length', truncation=True, is_split_into_words=True)\n",
    "    word_ids = tokenized_input.word_ids()\n",
    "    for i, label in enumerate(word_ids):\n",
    "        if label is None:\n",
    "            word_ids[i] = 7\n",
    "        else:\n",
    "            word_ids[i] = label__[tag[label]]\n",
    "    tokenized_input[\"labels\"] = word_ids\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d97bab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input_train = {\n",
    "    'input_ids': [],\n",
    "    'attention_mask': [],\n",
    "    'tags_bengali': []\n",
    "}\n",
    "\n",
    "for index in range(len(IHQID_train['question_bengali'])):\n",
    "    process_output = process_queries(IHQID_train['question_bengali'][index], IHQID_train['tags_bengali'][index])\n",
    "    encoded_input_train['input_ids'].append(process_output['input_ids'])\n",
    "    encoded_input_train['attention_mask'].append(process_output['attention_mask'])\n",
    "    encoded_input_train['tags_bengali'].append(process_output['labels'])\n",
    "\n",
    "encoded_input_test = {\n",
    "    'input_ids': [],\n",
    "    'attention_mask': [],\n",
    "    'tags_bengali': []\n",
    "}\n",
    "\n",
    "for index in range(len(IHQID_test['question_bengali'])):\n",
    "    process_output = process_queries(IHQID_test['question_bengali'][index], IHQID_test['tags_bengali'][index])\n",
    "    encoded_input_test['input_ids'].append(process_output['input_ids'])\n",
    "    encoded_input_test['attention_mask'].append(process_output['attention_mask'])\n",
    "    encoded_input_test['tags_bengali'].append(process_output['labels'])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(encoded_input_train['input_ids']).to(device),\n",
    "        torch.tensor(encoded_input_train['attention_mask']).to(device),\n",
    "        torch.tensor(encoded_input_train['tags_bengali']).to(device)\n",
    "    ),\n",
    "    batch_size=hyper_parameters['batch_size']\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(encoded_input_test['input_ids']).to(device),\n",
    "        torch.tensor(encoded_input_test['attention_mask']).to(device),\n",
    "        torch.tensor(encoded_input_test['tags_bengali']).to(device)\n",
    "    ),\n",
    "    batch_size=hyper_parameters['batch_size']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6359969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label__) + 1\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=hyper_parameters['lr']\n",
    ")\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "  \"linear\",\n",
    "  optimizer=optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=hyper_parameters['epochs'] * len(train_dataloader)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d42a8e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████████▌                                                                                                                                           | 1/10 [00:14<02:14, 14.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  - Training Loss: 0.1778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████████████████                                                                                                                            | 2/10 [00:29<01:58, 14.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2  - Training Loss: 0.0324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████████████████████████████████████████▌                                                                                                            | 3/10 [00:44<01:44, 14.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3  - Training Loss: 0.0242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████████████████████████                                                                                             | 4/10 [00:59<01:29, 14.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4  - Training Loss: 0.0182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████████████████████▌                                                                             | 5/10 [01:14<01:15, 15.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5  - Training Loss: 0.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████████████████████████████████████████████████                                                              | 6/10 [01:30<01:00, 15.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6  - Training Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                              | 7/10 [01:45<00:45, 15.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7  - Training Loss: 0.0096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                               | 8/10 [02:00<00:30, 15.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8  - Training Loss: 0.0081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌               | 9/10 [02:15<00:15, 15.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9  - Training Loss: 0.007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:31<00:00, 15.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10  - Training Loss: 0.0063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "updater = tqdm(range(hyper_parameters['epochs']))\n",
    "for epoch in updater:\n",
    "    total_train_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2],\n",
    "        }\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    print(\"Epoch:\", epoch + 1, \" - Training Loss:\", round(total_train_loss / len(train_dataloader), 4))\n",
    "\n",
    "\n",
    "# Didn't have much time to implement early stopping. So, saving the model at the end of all epochs.\n",
    "torch.save(model.state_dict(), f'ee_bridge_hi.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbcb90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "prediction = []\n",
    "gold_label = []\n",
    "\n",
    "extra_appended_tokens = 0\n",
    "\n",
    "for indexer, batch in enumerate(test_dataloader):\n",
    "\n",
    "    inputs = {\n",
    "        'input_ids': batch[0],\n",
    "        'attention_mask': batch[1],\n",
    "        'labels': batch[2],\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "\n",
    "    gold_label_cpu = inputs['labels'].cpu().numpy()\n",
    "    logits_vector = outputs.logits.detach().cpu().numpy()\n",
    "\n",
    "    assert(len(gold_label_cpu) == len(logits_vector))\n",
    "\n",
    "    for index in range(len(logits_vector)):\n",
    "        prediction_vector = []\n",
    "        for iterator__ in logits_vector[index].argmax(axis=1):\n",
    "            if iterator__ != 7:\n",
    "                prediction_vector.append(iterator__)\n",
    "                prediction.append(iterator__)\n",
    "        \n",
    "        gold_label_vector = []\n",
    "        for iterator__ in gold_label_cpu[index]:\n",
    "            if iterator__ != 7:\n",
    "                gold_label_vector.append(iterator__)\n",
    "                gold_label.append(iterator__)\n",
    "        \n",
    "        # There are some cases (only observed once) when there was one mismatch in vector of gold label and prediction\n",
    "        # To overcome that, for each tokenized sentence, append 0 to signify it not be classified as any of the entity\n",
    "        while len(gold_label) < len(prediction):\n",
    "            extra_appended_tokens += 1\n",
    "            gold_label.append(0)\n",
    "        \n",
    "        while len(prediction) < len(gold_label):\n",
    "            extra_appended_tokens += 1\n",
    "            prediction.append(0)\n",
    "\n",
    "print(\"Number of extra appended tokens : \", extra_appended_tokens)\n",
    "print(classification_report(gold_label, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744fa18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(gold_label, prediction)\n",
    "\n",
    "# Plot the confusion matrix.\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='g',\n",
    "    xticklabels=['O', 'B-treatment', 'I-treatment', 'B-disease', 'I-disease', 'B-drug', 'I-drug'],\n",
    "    yticklabels=['O', 'B-treatment', 'I-treatment', 'B-disease', 'I-disease', 'B-drug', 'I-drug']\n",
    ")\n",
    "plt.xlabel('Predicted Label', fontsize=10)\n",
    "plt.ylabel('Actual Label', fontsize=10)\n",
    "plt.title('Confusion Matrix', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7278f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e159f656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
